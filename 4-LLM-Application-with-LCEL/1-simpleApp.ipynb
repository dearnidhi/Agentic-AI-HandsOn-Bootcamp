{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3682cb9e",
   "metadata": {},
   "source": [
    "# Build Your First LLM App with LCEL\n",
    "\n",
    "In this quickstart, we’ll walk through building a simple LLM-powered application using LangChain. The app will take English text and translate it into another language.\n",
    "\n",
    "This is a beginner-friendly project — it’s just one LLM call with a prompt. But don’t be fooled! Even with something this simple, you’ll get a solid introduction to LangChain. Many powerful applications are built from just prompts and LLM calls.\n",
    "\n",
    "By the end of this tutorial, you’ll understand how to:\n",
    "\n",
    "Work with language models\n",
    "\n",
    "Create and use PromptTemplates and OutputParsers\n",
    "\n",
    "Combine components with LangChain Expression Language (LCEL)\n",
    "\n",
    "Debug and trace your app using LangSmith\n",
    "\n",
    "Deploy your app with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0a002",
   "metadata": {},
   "source": [
    "Direct invoke = talk to model\n",
    "\n",
    "Parser = clean reply\n",
    "\n",
    "Chain = connect steps\n",
    "\n",
    "Prompt messages = see inputs\n",
    "\n",
    "Final chain = full automated pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecaa4e1",
   "metadata": {},
   "source": [
    "# LCEL = LangChain Expression Language\n",
    "\n",
    "It’s like a mini programming language inside LangChain that makes it easy to connect and compose different components (models, prompts, parsers, retrievers, etc.) into one workflow.\n",
    "\n",
    "Think of it as “glue code” for LLM apps.\n",
    "\n",
    "# What LCEL does:\n",
    "\n",
    "Lets you chain components together (e.g., prompt → model → parser → output).\n",
    "\n",
    "Makes your pipeline declarative (you describe what should happen, not write lots of boilerplate code).\n",
    "\n",
    "Adds built-in tracing, streaming, and async support without extra code.\n",
    "\n",
    "Keeps your app modular so you can swap out models, prompts, or parsers easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b2c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.0.0-py3-none-any.whl (955 kB)\n",
      "     ---------------------------------------- 0.0/955.5 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 71.7/955.5 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 471.0/955.5 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  952.3/955.5 kB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 955.5/955.5 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.11.0-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: tqdm, jiter, openai\n",
      "Successfully installed jiter-0.11.0 openai-2.0.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open AI API Key and Open Source models--Llama3,Gemma2,mistral--Groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "#groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d610a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain_openai) (0.3.76)\n",
      "Collecting openai<2.0.0,>=1.104.2\n",
      "  Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "     ---------------------------------------- 0.0/948.6 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 71.7/948.6 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 553.0/948.6 kB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  942.1/948.6 kB 8.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 948.6/948.6 kB 6.7 MB/s eta 0:00:00\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Using cached tiktoken-0.11.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.31)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.9)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2025.9.18-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.25.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\window 11\\desktop\\agentic ai\\venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.104.2->langchain_openai) (0.4.6)\n",
      "Installing collected packages: regex, tiktoken, openai, langchain_openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 2.0.0\n",
      "    Uninstalling openai-2.0.0:\n",
      "      Successfully uninstalled openai-2.0.0\n",
      "Successfully installed langchain_openai-0.3.33 openai-1.109.1 regex-2025.9.18 tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955d7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e015c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔑 API Keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a7561",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b2c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Initialize Models\n",
    "# OpenAI GPT\n",
    "openai_model = ChatOpenAI(model = \"gpt-4o-min\", api_key =openai_api_key )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ab6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq with Llama3 (you can also try gemma2-9b-it or mistral-7b)\n",
    "#groq_model = ChatGroq(model=\"llama3-8b-8192\", api_key=groq_api_key)\n",
    "groq_model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ae29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq Result: **Hello**  - Bonjour\n",
      "\n",
      "**How are you?** - Comment allez-vous? \n",
      "\n",
      "(You can also use the more informal: **Comment ça va?**) \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other phrases you'd like me to translate!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 🟢 Example 1: Direct messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "result = groq_model.invoke(messages)\n",
    "print(\"Groq Result:\", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec3208f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parser: **Hello**  - Bonjour\n",
      "\n",
      "**How are you?** - Comment allez-vous? \n",
      "\n",
      "(You can also use the more informal: **Comment ça va?**) \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other phrases you'd like me to translate!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 🟢 Example 2: Using Parser\n",
    "parser= StrOutputParser()\n",
    "parsed_result = parser.invoke(result)\n",
    "print(\"parser:\", parsed_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510f0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chained Bonjour, comment allez-vous ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 🟢 Example 3: Using LCEL chaining\n",
    "chain = groq_model | parser\n",
    "print(\"chained\", chain.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862c025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_template = \"Translate the following into {language} :\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15c9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 🟢 Example 4: Prompt Templates + LCEL\n",
    "generic_template = \"Translate the following into {language} :\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    (\"system\",generic_template),\n",
    "    (\"user\",\"{text}\")\n",
    "    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbfa4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt messages: [SystemMessage(content='Translate the following into French :', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "result = prompt.invoke({\"language\" : \"French\", \"text\":\"Hello\"})\n",
    "print(\"prompt messages:\", result.to_messages())\n",
    "print(\"____________________________________________\")\n",
    "\n",
    "# Full chain with prompt + model + parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "554f85cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final chain: こんにちは (Konnichiwa) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | groq_model | parser\n",
    "print(\"final chain:\", chain.invoke({\"language\": \"japanese\" , \"text\": \"Hello\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a1683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
